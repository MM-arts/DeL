{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LEQEe0luCvEj"
   },
   "source": [
    "# 全人類がわかるディープラーニング Day5演習\n",
    "\n",
    "## 概要\n",
    "\n",
    "本演習では自然言語処理に代表される系列データを扱うためのネットワークであるRNNやその派生ネットワークによる学習を穴埋め形式で実装します。なお、予め用意されたコードはそのまま使用し、指示された穴埋め部を編集してください。\n",
    "演習問題文は<font color=\"Red\">赤字</font>です。このファイルは必ず最後までコードをすべて実行し、「最後までコードが実行可能」・「学習結果の出力がある」・「学習が成功している」の３つを満たした状態で提出してください。\n",
    "\n",
    "所要時間：3~8時間"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sm7ZvF2SCvEk"
   },
   "source": [
    "### ライブラリのインポート\n",
    "\n",
    "必要なライブラリをインポートします。エラーになる場合は該当するものをインストールしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-thDJ_VCvEl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# 乱数シードを指定\n",
    "np.random.seed(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2zc3aStCvEn"
   },
   "source": [
    "## 基本的関数、アルゴリズムの実装\n",
    "\n",
    "- SGD\n",
    "- Adam\n",
    "- sigmoid\n",
    "- softmax\n",
    "- clip_grads\n",
    "    勾配クリッピング用の関数。実装自体は単純なので省略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_dM-1Xrc-Aye"
   },
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        out = np.dot(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        x = self.x\n",
    "        N, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        db = np.sum(dout, axis=0)\n",
    "        dW = np.dot(x.T, dout)\n",
    "        dx = np.dot(dout, W.T)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RCuXbNbACvEo"
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]\n",
    "            \n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "            \n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-iaf2EbCvEp"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQF7Uc98CvFM"
   },
   "outputs": [],
   "source": [
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "icm4E44X-Ayo"
   },
   "source": [
    "## 自然言語処理用のレイヤークラス定義\n",
    "### Embedding クラス\n",
    "- 自然言語処理において、単語をベクトル（分散表現）に変換する前処理(Embeddingと言われる)が必須となる。\n",
    "- Embeddingクラスは入力されてきた単語列をべクトルに変換する。\n",
    "- 単語はID化されているため、一つのデータ（文章）は`[3,0,4,1]`のようになっている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sg0VFsMT-Ayp"
   },
   "source": [
    "例えば\n",
    "\n",
    "単語 0 を `[0.5, 1.2]`\n",
    "\n",
    "単語 1 を`[-1.4, 0.7]`\n",
    "\n",
    "単語 2 を` [2.5, -0.9]`\n",
    "\n",
    "単語 3 を`[5.6, 9.8]`\n",
    "\n",
    "単語 4 を`[-2.3, -0.8]`\n",
    "\n",
    "に変換するようなEmbed 層を用意する場合、\n",
    "\n",
    "$\n",
    "\\begin{pmatrix}\n",
    "0.5 & 1.2 \\\\\n",
    "-1.4 & 0.7 \\\\\n",
    "2.5 & -0.9 \\\\\n",
    "5.6 & 9.8 \\\\\n",
    "-2.3 & -0.8 \\\\\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "のような変換行列を用意し、入力の単語IDをindexとして各単語を変換してやれば良い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zemct8OW-Ayq"
   },
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XBqVDDVq-Ayt"
   },
   "source": [
    "逆伝播については、変換行列に対して、入力時の単語に相当する箇所に伝播してきた勾配を足し合わせてやれば良いため、`np.add.at` を使用すれば良い。\n",
    "\n",
    "`np.add.at(dW, self.idx, dout)` によって、`dW[self.idx]`に対して`dout`が加えられる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LdNoWAOa-Ayu",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.6,  9.8],\n",
       "       [ 0.5,  1.2],\n",
       "       [-2.3, -0.8],\n",
       "       [-1.4,  0.7]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([[0.5, 1.2],\n",
    "              [-1.4, 0.7],\n",
    "              [2.5, -0.9],\n",
    "              [5.6, 9.8],\n",
    "              [-2.3, -0.8]])\n",
    "embedder_example = Embedding(w)\n",
    "minibatch_example = np.array([3,0,4,1])\n",
    "embedder_example.forward(minibatch_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1SI5FXlf-Ayw"
   },
   "source": [
    "## Time層の実装\n",
    "RNNで系列データを扱う際、例えば時刻tでの入力データ$x_t$を順伝播させたのちに隠れ層の値$h_t$を得、次の時刻t+1での入力データ$x_{t+1}$を順伝播させ……というように、データを時刻ごとに分割してからRNN層やAffine層、Embedding層に入力するという形式を取らなくてはならず、系列データを一々分割し、各時刻データに対して順伝播をさせるというように手間がかかってしまう。\n",
    "\n",
    "そのため、全時刻に渡って一度に順伝播などの処理を行ってくれるように、ユニットを時刻方向に繋げ一つのユニットとすることを考える。\n",
    "ここではこのように時系列データをまとめて扱う層のことを`TimeEmbedding`や`TimeAffine`のように、各クラス名に`Time`を付けて表すことにする。\n",
    "\n",
    "<img src= time_layer_image.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_FnjBns-Ayx"
   },
   "source": [
    "### TimeEmbedding クラス\n",
    "\n",
    "TimeEmbedding クラスの入力として想定する形式は、\n",
    "\n",
    "（バッチサイズ）×（単語数=時刻数）\n",
    "\n",
    "であり、各単語をD次元ベクトルに変換するとすれば出力は\n",
    "\n",
    "（バッチサイズ）×（単語数=時刻数）×（次元数D）\n",
    "\n",
    "となる。\n",
    "\n",
    "系列に発展させた形としては、順伝播/逆伝播ともに時刻数tをfor文で順に処理してやれば良い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MO8XOzIWCvEr"
   },
   "outputs": [],
   "source": [
    "class TimeEmbedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.layers = None\n",
    "        self.W = W\n",
    "\n",
    "    def forward(self, xs):\n",
    "        N, T = xs.shape\n",
    "        V, D = self.W.shape\n",
    "\n",
    "        out = np.empty((N, T, D), dtype='f')\n",
    "        self.layers = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Embedding(self.W)\n",
    "            out[:, t, :] = layer.forward(xs[:, t])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, D = dout.shape\n",
    "\n",
    "        grad = 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            layer.backward(dout[:, t, :])\n",
    "            grad += layer.grads[0]\n",
    "\n",
    "        self.grads[0][...] = grad\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0YWRoPBX-Ay1"
   },
   "source": [
    "### TimeAffine クラス\n",
    "TimeAffineクラスの入力として想定する形式は、\n",
    "\n",
    "（バッチサイズ）×（単語数=時刻数）×（単語ベクトル次元数）\n",
    "\n",
    "であり、このAffineクラスによって単語ベクトル次元数$D_1$から$D_2$に変換すると考えれば、サイズ$D_1 \\times D_2$ の重み行列を用意し、\n",
    "\n",
    "（バッチサイズ）×（単語数=時刻数）×（$D_2$）\n",
    "\n",
    "が出力となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8oo7Gr_CvEu"
   },
   "outputs": [],
   "source": [
    "class TimeAffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        rx = x.reshape(N*T, -1)\n",
    "        out = np.dot(rx, W) + b\n",
    "        self.x = x\n",
    "        return out.reshape(N, T, -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        x = self.x\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        dout = dout.reshape(N*T, -1)\n",
    "        rx = x.reshape(N*T, -1)\n",
    "\n",
    "        db = np.sum(dout, axis=0)\n",
    "        dW = np.dot(rx.T, dout)\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dx = dx.reshape(*x.shape)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hvSTZU-1-Ay5"
   },
   "source": [
    "### TimeSoftmaxWithLoss クラス\n",
    "TimeSoftmaxWithLoss クラスの入力として想定する形式は、\n",
    "\n",
    "（バッチサイズ）×（単語数=時刻数）×（次元数）\n",
    "\n",
    "であり、この次元数についてsoftmax関数により変換を行うものと考える。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l3Sr6xoPCvEs"
   },
   "outputs": [],
   "source": [
    "class TimeSoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T, V = xs.shape\n",
    "\n",
    "        if ts.ndim == 3:  # 教師ラベルがone-hotベクトルの場合\n",
    "            ts = ts.argmax(axis=2)\n",
    "\n",
    "        # バッチ分と時系列分をまとめる（reshape）\n",
    "        xs = xs.reshape(N * T, V)\n",
    "        ts = ts.reshape(N * T)\n",
    "\n",
    "        ys = softmax(xs)\n",
    "        ls = np.log(ys[np.arange(N * T), ts])\n",
    "        loss = -np.sum(ls)\n",
    "        loss /= len(ts)\n",
    "\n",
    "        self.cache = (ts, ys, (N, T, V))\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ts, ys, (N, T, V) = self.cache\n",
    "\n",
    "        dx = ys\n",
    "        dx[np.arange(N * T), ts] -= 1\n",
    "        dx *= dout\n",
    "        dx /= len(ys)\n",
    "\n",
    "        dx = dx.reshape((N, T, V))\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIYsJAPZCvEy"
   },
   "source": [
    "## データセット用意\n",
    "\n",
    "ptbという英語の文章を集めたデータセットを用意します。\n",
    "\n",
    "このデータセットについて、単語を順に入力していき、次の単語を予測するというタスクを解かせてみます。\n",
    "本来であれば訓練データで学習をさせた後テストデータで検証を行いますが、簡単のため今回は学習のみを行い、テストによる検証は行いません。\n",
    "\n",
    "こちらのデータセットを用いて解いていくタスクは実践的なタスクと言うよりは、RNNの実装がうまくいっており、学習が成功することを確認するためのものとなります。\n",
    "\n",
    "データセットの用意の部分ですので、コードは読み飛ばしていただいても構いません。\n",
    "\n",
    "行っている処理の流れを示すと、\n",
    "1. データのダウンロード\n",
    "1. 単語とIDの変換ディクショナリの作成\n",
    "1. ダウンロードしてきた単語列のデータに対し、全単語をIDに変換\n",
    "\n",
    "と言う流れになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e8psB7A9CvEz"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import urllib.request\n",
    "import pickle\n",
    "\n",
    "url_base = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/'\n",
    "key_file = {\n",
    "    'train':'ptb.train.txt'\n",
    "}\n",
    "save_file = {\n",
    "    'train':'ptb.train.npy'\n",
    "}\n",
    "vocab_file = 'ptb.vocab.pkl'\n",
    "\n",
    "def _download(file_name):\n",
    "    file_path = './' + file_name\n",
    "    if os.path.exists(file_path):\n",
    "        return\n",
    "\n",
    "    print('Downloading ' + file_name + ' ... ')\n",
    "\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "    except urllib.error.URLError:\n",
    "        import ssl\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "\n",
    "    print('Done')\n",
    "\n",
    "def load_vocab():\n",
    "    vocab_path = './' + vocab_file\n",
    "\n",
    "    if os.path.exists(vocab_path):\n",
    "        with open(vocab_path, 'rb') as f:\n",
    "            word_to_id, id_to_word = pickle.load(f)\n",
    "        return word_to_id, id_to_word\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    data_type = 'train'\n",
    "    file_name = key_file[data_type]\n",
    "    file_path = './' + file_name\n",
    "\n",
    "    _download(file_name)\n",
    "\n",
    "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word not in word_to_id:\n",
    "            tmp_id = len(word_to_id)\n",
    "            word_to_id[word] = tmp_id\n",
    "            id_to_word[tmp_id] = word\n",
    "\n",
    "    with open(vocab_path, 'wb') as f:\n",
    "        pickle.dump((word_to_id, id_to_word), f)\n",
    "\n",
    "    return word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9nT8sbgCvE1"
   },
   "outputs": [],
   "source": [
    "def load_ptb(data_type='train'):\n",
    "    save_path = './' + save_file[data_type]\n",
    "    word_to_id, id_to_word = load_vocab()\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        corpus = np.load(save_path)\n",
    "        return corpus, word_to_id, id_to_word\n",
    "\n",
    "    file_name = key_file[data_type]\n",
    "    file_path = './' + file_name\n",
    "    _download(file_name)\n",
    "\n",
    "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    np.save(save_path, corpus)\n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rbYK-qb0-AzF"
   },
   "source": [
    "読み込んだデータを学習用に分割します。\n",
    "そのままではデータ数が大きすぎるので、前から1000単語のみを使用します。\n",
    "\n",
    "xsには0番目から998番目までの単語IDが、tsには1番目から999番目までの単語IDが格納されています。解くべきタスクとしては、xsのi番目までの単語IDを入力として、i+1番目の単語、即ちtsのi番目の単語を予測するというものになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mM4f-cue-AzG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size: 1000, vocabulary size: 418\n"
     ]
    }
   ],
   "source": [
    "# 学習データの読み込み（データセットを小さくする）\n",
    "corpus, word_to_id, id_to_word = load_ptb('train')\n",
    "corpus_size = 1000\n",
    "corpus = corpus[:corpus_size]\n",
    "vocab_size = int(max(corpus) + 1)\n",
    "\n",
    "xs = corpus[:-1]  # 入力\n",
    "ts = corpus[1:]  # 出力（教師ラベル）\n",
    "data_size = len(xs)\n",
    "print('corpus size: %d, vocabulary size: %d' % (corpus_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ii6uCudrCvE4"
   },
   "source": [
    "## ネットワーク定義\n",
    "### RNNクラス\n",
    "問1-1. <font color=\"Red\">RNNレイヤーを表すクラス RNNクラスを完成させてください。</font>\n",
    "\n",
    "  RNNクラスは以下に従って定義します。\n",
    "    \n",
    "  - `Wx`, `Wh`はそれぞれ順伝播時に入力、前時刻隠れ層にかかる重み行列\n",
    "  - 活性化関数は `tanh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UKhQecjFCvE4"
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh, b = self.params \n",
    "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b ######問1.1.1######\n",
    "        h_next = np.tanh(t) \n",
    "\n",
    "        self.cache = (x, h_prev, h_next)\n",
    "        return h_next\n",
    "\n",
    "    def backward(self, dh_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, h_next = self.cache\n",
    "\n",
    "        dt = dh_next * (1 - h_next**2) ######問1.1.3######\n",
    "        db = np.sum(dt, axis=0) ######問1.1.4######\n",
    "        dWh = np.dot(h_prev.T, dt) ######問1.1.5######\n",
    "        dh_prev = np.dot(dt, Wh.T) ######問1.1.6######\n",
    "        dWx = np.dot(x.T, dt) ######問1.1.7######\n",
    "        dx = np.dot(dt, Wx.T) ######問1.1.8######\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        return dx, dh_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qV66lPCN-AzM"
   },
   "source": [
    "### TimeRNNクラス\n",
    "問1-2. <font color=\"Red\">TimeRNNクラスを完成させてください。</font>\n",
    "\n",
    "TimeRNNクラスの仕様は以下の通りです。\n",
    "  \n",
    "  1. 順伝播\n",
    "      \n",
    "      - 時刻`t`を時系列分だけfor文でループさせます\n",
    "      - 順伝播により計算した隠れ層をメンバ変数`h`として格納します\n",
    "      - 順伝播時には<font color=\"Red\">メンバ変数`h`と入力データのうち時刻`t`に対応するデータを使用して</font>出力を計算します\n",
    "      \n",
    "  1. 逆伝播\n",
    "      - 今回のタスクにおいては、全時刻において次単語を予測し、その予測について損失が計算される、即ち勾配が入力されてくるため、逆伝播の入力`dhs`の想定される形は（バッチサイズ）×（単語数=時刻数）×（RNNの出力次元数）となります\n",
    "      - 各時刻において、<font color=\"Red\">次の時刻から伝播してきた勾配と現時刻における出力から伝播してきた勾配との和</font>を入力としてRNN層の逆伝播を計算します\n",
    "  \n",
    "また、今回のタスクにおける少々特殊な処理に対応するためや、この後実装する別タスクにおいても汎用的に使用するためにいくつか細かい仕様を加えます。\n",
    "実装上の細かい仕様のため、問題にはしていませんが、参考にしてください。\n",
    "\n",
    "  1. stateful\n",
    "    - 今回のタスクにおける目的はRNNネットワークが正しく実装できていることです。簡単に学習が成功していることを確認するため、（実務的にも用いられることのある）Truncated BPTT という少々特殊な処理を行います。\n",
    "    - 今回のタスクでは、入力する単語数（時刻数）として1000程度を入力していますが、この単語全てを順伝播させて逆伝播を計算し、ようやくパラメータ更新を1度行うというのでは効率が悪いため、順伝播は1000単語で行うものの逆伝播を5単語程度で区切って行う Truncated BPTT という方式を取っています。\n",
    "      1. まず、連続する5単語（例えば1~5番）を入力として順伝播・逆伝播・パラメータ更新を行います。\n",
    "      1. その際、順伝播の最終時刻における隠れ層`h`の値をメンバ変数として保持しておきます。\n",
    "      1. 次に5単語（先の例では6~10番）を入力として順伝播・逆伝播・パラメータ更新を行いますが、その際に初期入力の`h`を0で初期化せず、前から保持しているメンバ変数`h`の値をそのまま使用します。\n",
    "      1. このことにより、逆伝播は5単語単位で行われますが、順伝播は最初の単語から順にずっと計算されてきた値を使用することができます。\n",
    "      1. `stateful`がTrueの際にこのメンバ変数`h`の保持を行います。Falseの場合には`forward`関数が呼ばれるたびに`h`がリセット、即ち0で初期化されます。\n",
    "    \n",
    "  1. set_state\n",
    "    - 内部状態`h`を外から設定するための関数です。\n",
    "  \n",
    "  1. reset_state\n",
    "    - 内部状態`h`を外から削除するための関数です。\n",
    "  \n",
    "  1. params や grads としてパラメータ・勾配をまとめる\n",
    "    - 後でネットワーククラスを定義した際に、パラメータ更新などを楽に実行するための処理です。\n",
    "  1. backward における入力dhsの次元数による場合分け\n",
    "    - 今回のタスクでは全時刻において勾配が逆伝播してくるため、`dhs`の想定される形は（バッチサイズN）×（単語数=時刻数T）×（RNNの出力次元数H）となりますが、この後別のタスクを解く際には、全時刻ではなく最終時刻の出力のみから勾配が伝播してきます。\n",
    "    - この場合、dhsの次元数が違うため三次元に拡張することが必要になります。\n",
    "    - 最終時刻の出力のみから伝播してきたdhs の形は`(N,T,D)`のうち、`(N,D)`となっています。\n",
    "    - 最終時刻以外の勾配については何も伝播してきていないことから、全て0とすることで問題なく計算がなされます。\n",
    "    - 以上のことから、まず`np.zeros((N,T,D))`によって勾配の形を整え、その勾配の`[:,-1,:]`に対して入力された最終時刻における勾配を代入することで変わらず逆伝播を行うことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHobocVhCvE6"
   },
   "outputs": [],
   "source": [
    "class TimeRNN:\n",
    "    def __init__(self,input_size, output_size, stateful=False):\n",
    "        D, H = input_size, output_size\n",
    "        rn = np.random.randn\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # 重みの初期化\n",
    "        Wx = (rn(D, H) / np.sqrt(D)).astype('f')\n",
    "        Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
    "        b = np.zeros(H).astype('f')\n",
    "\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.dh = None, None\n",
    "        self.stateful = stateful\n",
    "        self.input_shapes = None\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        D, H = Wx.shape\n",
    "        self.input_shapes = [N,T,D]\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = RNN(*self.params)\n",
    "            self.h = layer.forward(xs[:,t,:], self.h) ######問1.2.1###### \n",
    "            hs[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = self.input_shapes\n",
    "        D, H = Wx.shape\n",
    "        \n",
    "        if dhs.ndim == 2:\n",
    "            temp = np.zeros((N,T,H))\n",
    "            temp[:,-1,:] = dhs\n",
    "            dhs = temp\n",
    "        \n",
    "        N, T, H = dhs.shape\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh = 0\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh = layer.backward(dhs[:,t,:] + dh) ######問1.2.2###### \n",
    "            dxs[:, t, :] = dx\n",
    "\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nDfIFVaw-AzQ"
   },
   "source": [
    "### SimpleRnnNetwork クラス\n",
    "バッチサイズN、単語数T、単語の種類数V、embedにより単語をベクトル化した際の次元数D、RNNによる出力次元数Hとします。\n",
    "\n",
    "入力されてくるデータのサイズは`(N,T)`であり\n",
    "\n",
    "TimeEmbedding層により`(N,T,D)`に変換\n",
    "\n",
    "TimeRNN層により`(N,T,H)`に変換\n",
    "\n",
    "TimeAffine層により`(N,T,V)`に変換した後、V次元の中で最大のものを予測単語IDとして出力します。\n",
    "損失については、Softmaxを使用してV種類の各単語について確率値を出力し、クロスエントロピーで計算します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XIN7sid8CvE8"
   },
   "outputs": [],
   "source": [
    "class SimpleRnnNetwork:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # 重みの初期化\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeRNN(D, H, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.rnn_layer = self.layers[1]\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        loss = self.loss_layer.forward(xs, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.rnn_layer.reset_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ET7tV8h1-AzU"
   },
   "source": [
    "### ハイパーパラメータや学習用データセットの用意\n",
    "\n",
    "バッチサイズ10のミニバッチを作成するため、offsets や time_idx などを使用します。\n",
    "細かい挙動まで追う必要はありませんが、Truncated BPTT を行うため、バッチサイズ10, 単語数5のミニバッチを作成したあと、通常のようにまたランダムに連続した5単語を取るのではなく、先のミニバッチから連続した5単語をミニバッチとして選択する必要があります。\n",
    "\n",
    "例として簡単のために単語数100個、バッチサイズ2とした場合を考えます。\n",
    "\n",
    "1エポック目でのミニバッチ選択を追うと、\n",
    "\n",
    "```\n",
    "0番目の単語-1番目の単語-2番目の単語-3番目の単語-4番目の単語\n",
    "49番目の単語-50番目の単語-51番目の単語-52番目の単語-53番目の単語\n",
    "```\n",
    "    ↓\n",
    "```\n",
    "5番目の単語-6番目の単語-7番目の単語-8番目の単語-9番目の単語\n",
    "54番目の単語-55番目の単語-56番目の単語-57番目の単語-58番目の単語\n",
    "```\n",
    "    ↓\n",
    "    ……\n",
    "    \n",
    "と言うようにミニバッチ内の各データについて連続して単語を選択しています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_OX0jXUwCvE9"
   },
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "batch_size = 20 #10\n",
    "wordvec_size = 100\n",
    "hidden_size = 100\n",
    "time_size = 5 \n",
    "lr = 10 #0.1\n",
    "max_epoch = 40\n",
    "max_grad = 0.25\n",
    "\n",
    "max_iters = data_size // (batch_size * time_size)\n",
    "\n",
    "optimizer = SGD(lr)\n",
    "\n",
    "jump = (corpus_size - 1) // batch_size\n",
    "offsets = [i * jump for i in range(batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Xt6DtJDCvFB"
   },
   "source": [
    "## 学習、評価\n",
    "perplexity で評価を行います。\n",
    "\n",
    "40エポックでperplexityが一桁程度まで低下していれば学習成功です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7cDXCBHZ-Azb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 | perplexity 383.29\n",
      "| epoch 2 | perplexity 404.50\n",
      "| epoch 3 | perplexity 491.92\n",
      "| epoch 4 | perplexity 376.12\n",
      "| epoch 5 | perplexity 733.03\n",
      "| epoch 6 | perplexity 445.26\n",
      "| epoch 7 | perplexity 441.94\n",
      "| epoch 8 | perplexity 397.05\n",
      "| epoch 9 | perplexity 400.56\n",
      "| epoch 10 | perplexity 412.79\n",
      "| epoch 11 | perplexity 365.66\n",
      "| epoch 12 | perplexity 279.37\n",
      "| epoch 13 | perplexity 336.01\n",
      "| epoch 14 | perplexity 343.69\n",
      "| epoch 15 | perplexity 280.76\n",
      "| epoch 16 | perplexity 280.55\n",
      "| epoch 17 | perplexity 209.95\n",
      "| epoch 18 | perplexity 183.33\n",
      "| epoch 19 | perplexity 160.38\n",
      "| epoch 20 | perplexity 125.45\n",
      "| epoch 21 | perplexity 94.64\n",
      "| epoch 22 | perplexity 68.83\n",
      "| epoch 23 | perplexity 53.40\n",
      "| epoch 24 | perplexity 39.26\n",
      "| epoch 25 | perplexity 31.40\n",
      "| epoch 26 | perplexity 23.63\n",
      "| epoch 27 | perplexity 19.72\n",
      "| epoch 28 | perplexity 14.86\n",
      "| epoch 29 | perplexity 12.27\n",
      "| epoch 30 | perplexity 10.44\n",
      "| epoch 31 | perplexity 8.23\n",
      "| epoch 32 | perplexity 6.58\n",
      "| epoch 33 | perplexity 5.33\n",
      "| epoch 34 | perplexity 5.24\n",
      "| epoch 35 | perplexity 4.14\n",
      "| epoch 36 | perplexity 3.97\n",
      "| epoch 37 | perplexity 3.73\n",
      "| epoch 38 | perplexity 3.20\n",
      "| epoch 39 | perplexity 3.05\n",
      "| epoch 40 | perplexity 2.73\n"
     ]
    }
   ],
   "source": [
    "model = SimpleRnnNetwork(vocab_size, wordvec_size, hidden_size)\n",
    "#model = LSTMNetwork(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "time_idx = 0\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "rnn_ppl_list = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for iter in range(max_iters):\n",
    "        # ミニバッチの取得\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
    "                batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
    "            time_idx += 1\n",
    "\n",
    "        # 勾配を求め、パラメータを更新\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        if max_grad is not None:\n",
    "            clip_grads(model.grads, max_grad)\n",
    "\n",
    "        optimizer.update(model.params, model.grads)\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "\n",
    "    # エポックごとにパープレキシティの評価\n",
    "    ppl = np.exp(total_loss / loss_count)\n",
    "    print('| epoch %d | perplexity %.2f'\n",
    "          % (epoch+1, ppl))\n",
    "    rnn_ppl_list.append(float(ppl))\n",
    "    total_loss, loss_count = 0, 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DyzEKNaLCvFD"
   },
   "source": [
    "## LSTM\n",
    "### LSTMクラス\n",
    "\n",
    "問2-1. <font color=\"Red\">LSTMクラスを完成させてください。</font>\n",
    "\n",
    "LSTMクラスの仕様はRNNクラスとほとんど同じです。\n",
    "\n",
    "各ゲートの計算と通常の順伝播の計算に使用するパラメータを行列にまとめ、一行で計算できるように実装しています。\n",
    "\n",
    "f,g,i,o はそれぞれ、忘却ゲート・入力からの順伝播・入力ゲート・出力ゲートを表しています。\n",
    "\n",
    "<img src=\"lstm_image.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "972frd2QCvFE"
   },
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, H = h_prev.shape\n",
    "\n",
    "        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b ######問2.1.1######\n",
    "\n",
    "        f = A[:, :H]\n",
    "        g = A[:, H:2*H]\n",
    "        i = A[:, 2*H:3*H]\n",
    "        o = A[:, 3*H:]\n",
    "\n",
    "        f = sigmoid(f) ######問2.1.2######\n",
    "        g = np.tanh(g) ######問2.1.3######\n",
    "        i = sigmoid(i) ######問2.1.4######\n",
    "        o = sigmoid(o) ######問2.1.5######\n",
    "\n",
    "        c_next = f * c_prev + g * i ######問2.1.6######\n",
    "        h_next = o * np.tanh(c_next) ######問2.1.7######\n",
    "\n",
    "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "    def backward(self, dh_next, dc_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
    "\n",
    "        tanh_c_next = np.tanh(c_next)\n",
    "\n",
    "        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2) ######問2.1.8######\n",
    "\n",
    "        dc_prev = ds * f ######問2.1.9######\n",
    "\n",
    "        di = ds * g\n",
    "        df = ds * c_prev\n",
    "        do = dh_next * tanh_c_next\n",
    "        dg = ds * i\n",
    "\n",
    "        di *= i * (1 - i)\n",
    "        df *= f * (1 - f)\n",
    "        do *= o * (1 - o)\n",
    "        dg *= (1 - g ** 2)\n",
    "\n",
    "        dA = np.hstack((df, dg, di, do))\n",
    "\n",
    "        dWh = np.dot(h_prev.T, dA)\n",
    "        dWx = np.dot(x.T, dA)\n",
    "        db = dA.sum(axis=0)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        dx = np.dot(dA, Wx.T)\n",
    "        dh_prev = np.dot(dA, Wh.T)\n",
    "\n",
    "        return dx, dh_prev, dc_prev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fSpv_YIw-Azh"
   },
   "source": [
    "### TimeLSTMクラス\n",
    "\n",
    "問2-2. <font color=\"Red\">TimeLSTMクラスを完成させてください。</font>\n",
    "\n",
    "TimeRNNと同じように実装します。\n",
    "\n",
    "SimpleRNN のときと違って、重みの初期化に注意を払う必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RXkWl14bCvFF"
   },
   "outputs": [],
   "source": [
    "class TimeLSTM:\n",
    "    def __init__(self, input_size, output_size, stateful=False):\n",
    "        D,H = input_size, output_size\n",
    "        rn = np.random.randn\n",
    "        np.random.seed(0)\n",
    "        \n",
    "        #Wx = (rn(######問2.2.1######) / np.sqrt(D)).astype('f') \n",
    "        Wx = (rn(D, 4*H)) / np.sqrt(D).astype('f') ######問2.2.1###### \n",
    "        #Wh = (rn(######問2.2.2######) / np.sqrt(H)).astype('f') \n",
    "        Wh = (rn(H, 4*H)) / np.sqrt(H).astype('f') ######問2.2.2###### \n",
    "        #b = np.zeros(######問2.2.3######).astype('f') \n",
    "        b = np.zeros(4*H).astype('f') ######問2.2.3######) \n",
    "\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.c = None, None\n",
    "        self.dh = None\n",
    "        self.stateful = stateful\n",
    "        self.input_shapes = None\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        H = Wh.shape[0]\n",
    "        self.input_shapes = [N,T,D]\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "        if not self.stateful or self.c is None:\n",
    "            self.c = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = LSTM(*self.params)\n",
    "            # self.h, self.c = layer.forward(######問2.2.4######) \n",
    "            self.h, self.c = layer.forward(xs[:,t,:],self.h, self.c) ######問2.2.4###### \n",
    "            hs[:, t, :] = self.h\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        H = Wh.shape[0]\n",
    "        N, T, D = self.input_shapes\n",
    "        \n",
    "        if dhs.ndim == 2:\n",
    "            temp = np.zeros((N,T,H))\n",
    "            temp[:,-1,:] = dhs\n",
    "            dhs = temp\n",
    "  \n",
    "        N, T, H = dhs.shape\n",
    " \n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh, dc = 0, 0\n",
    "\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            # dx, dh, dc = layer.backward(######問2.2.5######) \n",
    "            dx, dh, dc = layer.backward(dhs[:,t,:] + dh, dc) ######問2.2.5###### \n",
    "            dxs[:, t, :] = dx\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h, c=None):\n",
    "        self.h, self.c = h, c\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h, self.c = None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "STsi7N7c-Azk"
   },
   "source": [
    "### LSTMNetworkクラス\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9UIwu6fuCvFI"
   },
   "outputs": [],
   "source": [
    "class LSTMNetwork:\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # 重みの初期化\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeLSTM(D, H, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layer = self.layers[1]\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        score = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.lstm_layer.reset_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ofZZ-gIpCvFO"
   },
   "source": [
    "## 学習、評価\n",
    "\n",
    "ハイパーパラメータなどは先ほどのRNNと全て共通で学習させます。\n",
    "\n",
    "40エポックでperplexity が5以下となっていれば学習成功です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hcgBW6tmCvFP",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 | perplexity 278.48\n",
      "| epoch 2 | perplexity 224.88\n",
      "| epoch 3 | perplexity 205.25\n",
      "| epoch 4 | perplexity 188.97\n",
      "| epoch 5 | perplexity 175.05\n",
      "| epoch 6 | perplexity 155.77\n",
      "| epoch 7 | perplexity 133.43\n",
      "| epoch 8 | perplexity 106.73\n",
      "| epoch 9 | perplexity 81.01\n",
      "| epoch 10 | perplexity 58.26\n",
      "| epoch 11 | perplexity 40.60\n",
      "| epoch 12 | perplexity 27.62\n",
      "| epoch 13 | perplexity 20.00\n",
      "| epoch 14 | perplexity 14.09\n",
      "| epoch 15 | perplexity 10.32\n",
      "| epoch 16 | perplexity 7.90\n",
      "| epoch 17 | perplexity 5.75\n",
      "| epoch 18 | perplexity 4.58\n",
      "| epoch 19 | perplexity 3.62\n",
      "| epoch 20 | perplexity 3.06\n",
      "| epoch 21 | perplexity 2.63\n",
      "| epoch 22 | perplexity 2.23\n",
      "| epoch 23 | perplexity 1.98\n",
      "| epoch 24 | perplexity 1.73\n",
      "| epoch 25 | perplexity 1.67\n",
      "| epoch 26 | perplexity 1.54\n",
      "| epoch 27 | perplexity 1.43\n",
      "| epoch 28 | perplexity 1.38\n",
      "| epoch 29 | perplexity 1.29\n",
      "| epoch 30 | perplexity 1.23\n",
      "| epoch 31 | perplexity 1.22\n",
      "| epoch 32 | perplexity 1.17\n",
      "| epoch 33 | perplexity 1.18\n",
      "| epoch 34 | perplexity 1.18\n",
      "| epoch 35 | perplexity 1.10\n",
      "| epoch 36 | perplexity 1.07\n",
      "| epoch 37 | perplexity 1.06\n",
      "| epoch 38 | perplexity 1.07\n",
      "| epoch 39 | perplexity 1.05\n",
      "| epoch 40 | perplexity 1.05\n"
     ]
    }
   ],
   "source": [
    "model = LSTMNetwork(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "time_idx = 0\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "lstm_ppl_list = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for iter in range(max_iters):\n",
    "        # ミニバッチの取得\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
    "                batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
    "            time_idx += 1\n",
    "\n",
    "        # 勾配を求め、パラメータを更新\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        if max_grad is not None:\n",
    "            clip_grads(model.grads, max_grad)\n",
    "\n",
    "        optimizer.update(model.params, model.grads)\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "\n",
    "    # エポックごとにパープレキシティの評価\n",
    "    ppl = np.exp(total_loss / loss_count)\n",
    "    print('| epoch %d | perplexity %.2f'\n",
    "          % (epoch+1, ppl))\n",
    "    lstm_ppl_list.append(float(ppl))\n",
    "    total_loss, loss_count = 0, 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fnkwvXuS-Azs"
   },
   "source": [
    "LSTMとRNNの学習結果を比較し、LSTMの方が安定して素早く学習が収束していることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jBRkO629-Azt"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(max_epoch), rnn_ppl_list)\n",
    "plt.plot(range(max_epoch), lstm_ppl_list)\n",
    "plt.legend([\"RNN\", \"LSTM\"])\n",
    "plt.xlabel(\"#epoch\")\n",
    "plt.ylabel(\"perplexity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5a9St6Om-Azx"
   },
   "source": [
    "\n",
    "## 提出可否"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V4T4bucL-Azy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習成功です。次のステップに進んでください。\n"
     ]
    }
   ],
   "source": [
    "if rnn_ppl_list[-1] > 10:\n",
    "    print(\"RNNの実装に間違いがあります。問1を見直してください。\")\n",
    "elif lstm_ppl_list[-1] > 5:\n",
    "    print(\"LSTMの実装に間違いがあります。問2を見直してください。\")\n",
    "else:\n",
    "    print(\"学習成功です。次のステップに進んでください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q2FutIo3-Az0"
   },
   "source": [
    "## GRU\n",
    "### GRUクラス\n",
    "\n",
    "問3-1. <font color=\"Red\">GRUクラスを完成させてください。</font>\n",
    "\n",
    "GRUクラスの仕様はLSTMクラスとほとんど同じです。\n",
    "\n",
    "各ゲートの計算と通常の順伝播の計算に使用するパラメータを行列にまとめ、一行で計算できるように実装しています。\n",
    "\n",
    "r がリセットゲート、zが更新ゲートを表します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J3lrNddh-Az1"
   },
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    def __init__(self, Wx, Wh):\n",
    "        self.params = [Wx, Wh]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh = self.params\n",
    "        H, H3 = Wh.shape\n",
    "        Wxz, Wxr, Wx = Wx[:, :H], Wx[:, H:2 * H], Wx[:, 2 * H:]\n",
    "        Whz, Whr, Wh = Wh[:, :H], Wh[:, H:2 * H], Wh[:, 2 * H:]\n",
    "\n",
    "        z = sigmoid(np.dot(x, Wxz) + np.dot(h_prev, Whz))\n",
    "        r = sigmoid(np.dot(x, Wxr) + np.dot(h_prev, Whr))\n",
    "        h_hat = np.tanh(np.dot(x, Wx) + np.dot(r*h_prev, Wh))\n",
    "        h_next = (1-z) * h_prev + z * h_hat ######問3.1.1######\n",
    "\n",
    "        self.cache = (x, h_prev, z, r, h_hat)\n",
    "\n",
    "        return h_next\n",
    "\n",
    "\n",
    "    def backward(self, dh_next):\n",
    "        Wx, Wh = self.params\n",
    "        H, H3 = Wh.shape\n",
    "        Wxz, Wxr, Wx = Wx[:, :H], Wx[:, H:2 * H], Wx[:, 2 * H:]\n",
    "        Whz, Whr, Wh = Wh[:, :H], Wh[:, H:2 * H], Wh[:, 2 * H:]\n",
    "        x, h_prev, z, r, h_hat = self.cache\n",
    "\n",
    "        dh_hat =dh_next * z\n",
    "        dh_prev = dh_next * (1-z)\n",
    "\n",
    "        dt = dh_hat * (1 - h_hat ** 2)\n",
    "        dWh = np.dot((r * h_prev).T, dt)\n",
    "        dhr = np.dot(dt, Wh.T)\n",
    "        dWx = np.dot(x.T, dt)\n",
    "        dx = np.dot(dt, Wx.T)\n",
    "        dh_prev += r * dhr\n",
    "\n",
    "        dz = dh_next * h_hat - dh_next * h_prev\n",
    "        dt = dz * z * (1-z)\n",
    "        dWhz = np.dot(h_prev.T, dt)\n",
    "        dh_prev += np.dot(dt, Whz.T)\n",
    "        dWxz = np.dot(x.T, dt)\n",
    "        dx += np.dot(dt, Wxz.T)\n",
    "\n",
    "        dr = dhr * h_prev\n",
    "        dt = dr * r * (1-r)\n",
    "        dWhr = np.dot(h_prev.T, dt)\n",
    "        dh_prev += np.dot(dt, Whr.T)\n",
    "        dWxr = np.dot(x.T, dt)\n",
    "        dx += np.dot(dt, Wxr.T)\n",
    "\n",
    "        dWx = np.hstack((dWxz, dWxr, dWx))\n",
    "        dWh = np.hstack((dWhz, dWhr, dWh))\n",
    "        \n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "\n",
    "\n",
    "        return dx, dh_prev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t7TYSfoi-Az2"
   },
   "source": [
    "### TimeGRUクラス\n",
    "\n",
    "問3-2. <font color=\"Red\">TimeGRUクラスを完成させてください。</font>\n",
    "\n",
    "TimeGRUクラスの仕様はTimeLSTMクラスとほとんど同じです。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-jclSpgq-Az3"
   },
   "outputs": [],
   "source": [
    "class TimeGRU:\n",
    "    def __init__(self, input_size, output_size, stateful=False):\n",
    "        D, H = input_size, output_size\n",
    "        rn = np.random.randn\n",
    "        np.random.seed(0)\n",
    "        \n",
    "        #Wx = (rn(######問3.2.1######) / np.sqrt(D)).astype('f') \n",
    "        Wx = (rn(D, 3 * H) / np.sqrt(D)).astype('f') ######問3.2.1###### \n",
    "        #Wh = (rn(######問3.2.2######) / np.sqrt(H)).astype('f') \n",
    "        Wh = (rn(H, 3 * H) / np.sqrt(H)).astype('f') ######問3.2.2###### \n",
    "\n",
    "        self.params = [Wx, Wh]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh)]\n",
    "        self.layers = None\n",
    "        self.h = None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh = self.params\n",
    "        N, T, D = xs.shape\n",
    "        H, H3 = Wh.shape\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = GRU(*self.params)\n",
    "            # self.h = layer.forward(######問3.2.3######) \n",
    "            self.h = layer.forward(xs[:,t,:], self.h) ######問3.2.3###### \n",
    "            hs[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D = Wx.shape[0]\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        grads = [0,0]\n",
    "        dh = 0\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            #dx, dh = layer.backward(######問3.2.4######) \n",
    "            dx, dh = layer.backward(dhs[:,t,:] + dh) ######問3.2.4###### \n",
    "\n",
    "            dxs[:, t, :] = dx\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "            \n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxL7g9mu-Az5"
   },
   "source": [
    "### GRUNetworkクラス\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_vrYrBg--Az6"
   },
   "outputs": [],
   "source": [
    "class GRUNetwork:\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # 重みの初期化\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeGRU(D, H, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layer = self.layers[1]\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        score = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.lstm_layer.reset_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yxbAcqGa-Az8"
   },
   "source": [
    "## 学習、評価\n",
    "\n",
    "ハイパーパラメータなどは先ほどのRNN・LSTMと全て共通で学習させます。\n",
    "\n",
    "40エポックでperplexity が5以下となっていれば学習成功です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QL1EbV7v-Az9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 | perplexity 288.44\n",
      "| epoch 2 | perplexity 204.04\n",
      "| epoch 3 | perplexity 170.58\n",
      "| epoch 4 | perplexity 121.40\n",
      "| epoch 5 | perplexity 77.48\n",
      "| epoch 6 | perplexity 47.27\n",
      "| epoch 7 | perplexity 31.21\n",
      "| epoch 8 | perplexity 20.84\n",
      "| epoch 9 | perplexity 13.75\n",
      "| epoch 10 | perplexity 9.34\n",
      "| epoch 11 | perplexity 6.24\n",
      "| epoch 12 | perplexity 4.44\n",
      "| epoch 13 | perplexity 3.38\n",
      "| epoch 14 | perplexity 2.62\n",
      "| epoch 15 | perplexity 2.13\n",
      "| epoch 16 | perplexity 1.75\n",
      "| epoch 17 | perplexity 1.58\n",
      "| epoch 18 | perplexity 1.44\n",
      "| epoch 19 | perplexity 1.35\n",
      "| epoch 20 | perplexity 1.31\n",
      "| epoch 21 | perplexity 1.25\n",
      "| epoch 22 | perplexity 1.20\n",
      "| epoch 23 | perplexity 1.19\n",
      "| epoch 24 | perplexity 1.14\n",
      "| epoch 25 | perplexity 1.13\n",
      "| epoch 26 | perplexity 1.11\n",
      "| epoch 27 | perplexity 1.11\n",
      "| epoch 28 | perplexity 1.06\n",
      "| epoch 29 | perplexity 1.06\n",
      "| epoch 30 | perplexity 1.05\n",
      "| epoch 31 | perplexity 1.04\n",
      "| epoch 32 | perplexity 1.02\n",
      "| epoch 33 | perplexity 1.02\n",
      "| epoch 34 | perplexity 1.01\n",
      "| epoch 35 | perplexity 1.01\n",
      "| epoch 36 | perplexity 1.01\n",
      "| epoch 37 | perplexity 1.01\n",
      "| epoch 38 | perplexity 1.01\n",
      "| epoch 39 | perplexity 1.01\n",
      "| epoch 40 | perplexity 1.01\n"
     ]
    }
   ],
   "source": [
    "model = GRUNetwork(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "time_idx = 0\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "gru_ppl_list = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for iter in range(max_iters):\n",
    "        # ミニバッチの取得\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
    "                batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
    "            time_idx += 1\n",
    "\n",
    "        # 勾配を求め、パラメータを更新\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        if max_grad is not None:\n",
    "            clip_grads(model.grads, max_grad)\n",
    "\n",
    "        optimizer.update(model.params, model.grads)\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "\n",
    "    # エポックごとにパープレキシティの評価\n",
    "    ppl = np.exp(total_loss / loss_count)\n",
    "    print('| epoch %d | perplexity %.2f'\n",
    "          % (epoch+1, ppl))\n",
    "    gru_ppl_list.append(float(ppl))\n",
    "    total_loss, loss_count = 0, 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G80Gpmza-A0A"
   },
   "source": [
    "GRUとLSTM、RNNの学習結果を比較し、GRUがLSTMと同様（若しくはそれ以上）の性能を持っていることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nyRMCYbX-A0B",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(max_epoch), rnn_ppl_list)\n",
    "plt.plot(range(max_epoch), lstm_ppl_list)\n",
    "plt.plot(range(max_epoch), gru_ppl_list)\n",
    "plt.legend([\"RNN\", \"LSTM\", \"GRU\"])\n",
    "plt.xlabel(\"#epoch\")\n",
    "plt.ylabel(\"perplexity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8-QQl5QZ-A0D"
   },
   "source": [
    "\n",
    "## 提出可否"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lpWhGBYN-A0D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習成功です。次のステップに進んでください。\n"
     ]
    }
   ],
   "source": [
    "if gru_ppl_list[-1] > 5:\n",
    "    print(\"GRUの実装に間違いがあります。問3を見直してください。\")\n",
    "else:\n",
    "    print(\"学習成功です。次のステップに進んでください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g9ZeqQfS-A0F"
   },
   "source": [
    "## Classification\n",
    "では実際にRNNやLSTMを用いて実践的な問題を解いていきましょう。\n",
    "\n",
    "今回解いていくタスクは二値分類なので、まずは出力層の活性化と損失関数をつなげたSigmoidWithLossクラスを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vrQMLkm-A0F"
   },
   "outputs": [],
   "source": [
    "class SigmoidWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        x = x.flatten()\n",
    "        self.y = sigmoid(x)\n",
    "        self.loss = -np.mean(np.log(self.y + 1e-7) * t + np.log(1 - self.y + 1e-7) * (1 - t))\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t)/batch_size\n",
    "        dx = dx.reshape(-1,1)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p0r8DMHJ-A0H"
   },
   "source": [
    "## データセットの用意\n",
    "\n",
    "今回使用するデータは \"movie review\" と呼ばれる映画の評論記事の分類問題です。\n",
    "評論記事に対して「1(positive)」「0(negative)」の二値がラベルとして付与されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hBxRZA55-A0H"
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FYaC0Zar0VIG"
   },
   "outputs": [],
   "source": [
    "def load_movie_reviews():\n",
    "    from nltk.corpus import movie_reviews\n",
    "    try:\n",
    "        movie_reviews.categories()\n",
    "    except:\n",
    "        import nltk\n",
    "        nltk.download('movie_reviews')\n",
    "        from nltk.corpus import movie_reviews\n",
    "    raw_data = []\n",
    "\n",
    "    # NLTK's corpus is structured in an interesting way\n",
    "    # first iterate through the two categories (pos and neg)\n",
    "    for category in movie_reviews.categories():\n",
    "\n",
    "        if category == 'pos':\n",
    "            label = '1'\n",
    "        elif category == 'neg':\n",
    "            label = '0'\n",
    "\n",
    "        # each of these categories is just fileids, so grab those\n",
    "        for fileid in movie_reviews.fileids(category):\n",
    "            # then each review is a NLTK class where each item in that class instance is a word\n",
    "            review_words = list(movie_reviews.words(fileid))\n",
    "            if len(review_words) >= 400:\n",
    "                review_words = review_words[:400]\n",
    "            else:\n",
    "                review_words.extend([\" \" for i in range(400 - len(review_words))])\n",
    "            review_dictionary = {\n",
    "                'text': review_words,\n",
    "                'label': label\n",
    "            }\n",
    "\n",
    "            raw_data.append(review_dictionary)\n",
    "\n",
    "    return raw_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z3Q0F0VK-A0M"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Masashi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"data.pkl\"):\n",
    "    f = open(\"data.pkl\", \"rb\")\n",
    "    data = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    data = load_movie_reviews()\n",
    "    f = open(\"data.pkl\", \"wb\")\n",
    "    pickle.dump(data,f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WUPxiQj4-A0O"
   },
   "source": [
    "## Embedder の用意\n",
    "\n",
    "先ほどの例ではEmbed層をネットワーク内に含めていましたが、Embed 層も含めて学習をすると時間がかかってしまうため、今回はあらかじめ用意された embedder を使用してネットワークに入れる前に単語列をベクトル化しておきます。\n",
    "\n",
    "embedderモデルのダウンロードには時間がかかりますのでご注意ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z6IvEiQ5-A0R"
   },
   "outputs": [],
   "source": [
    "if os.path.exists(\"embedder.model\"):\n",
    "    model = word2vec.Word2VecKeyedVectors.load_word2vec_format(\"embedder.model\")\n",
    "else:\n",
    "    model = api.load(\"glove-twitter-25\")\n",
    "    model.save_word2vec_format(\"embedder.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oocxBSPH-A0T"
   },
   "outputs": [],
   "source": [
    "def embed_one_word_via_model(word, model):\n",
    "    try:\n",
    "        return model[word]\n",
    "    except:\n",
    "        return np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vy8BecrW-A0V",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding = []\n",
    "labels = []\n",
    "for d in data:\n",
    "    embedding.append(np.array([embed_one_word_via_model(word,model) for word in d[\"text\"]]))\n",
    "    labels.append(int(d[\"label\"]))\n",
    "embedding = np.array(embedding)\n",
    "labels = np.array(labels)\n",
    "\n",
    "X_train, X_test, T_train, T_test = train_test_split(embedding, labels, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XtARJu9j-A0X"
   },
   "source": [
    "## ネットワーク の用意\n",
    "問4-1. <font color=\"Red\">以下の SimpleRNNClassifier, LSTMClassifier クラスを完成させてください。</font>\n",
    "\n",
    "先ほどの問題では各時刻の入力に対して一つずつ出力が計算されましたが、今回のタスクにおいては、全時刻の入力データに対して一つの出力を計算します。\n",
    "\n",
    "そのため、順伝播においてRNN層やLSTM層の出力に対して、時系列のうち最終出力のみを取り出し、Affine層に入力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WXX5-R79-A0Y"
   },
   "outputs": [],
   "source": [
    "class SimpleRNNClassifier:\n",
    "    def __init__(self, wordvec_size, hidden_size):\n",
    "        D, H = wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # 重みの初期化\n",
    "        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')\n",
    "        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
    "        rnn_b = np.zeros(H).astype('f')\n",
    "        affine_W = (rn(H, 1) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(1).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.affine_layer = Affine(affine_W, affine_b)\n",
    "        self.loss_layer = SigmoidWithLoss()\n",
    "        self.rnn_layer = TimeRNN(D, H, stateful=False)\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        self.params += self.rnn_layer.params\n",
    "        self.grads += self.rnn_layer.grads\n",
    "        self.params += self.affine_layer.params\n",
    "        self.grads += self.affine_layer.grads\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        xs = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(xs, ts)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, xs):\n",
    "        xs = self.rnn_layer.forward(xs)[######問4.1.1######] \n",
    "        xs = self.affine_layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        dout = self.affine_layer.backward(dout)\n",
    "        dout = self.rnn_layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.rnn_layer.reset_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kYFDYsE7-A0b"
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier:\n",
    "    def __init__(self, wordvec_size, hidden_size):\n",
    "        D, H = wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # 重みの初期化\n",
    "        rnn_Wx = (rn(D, 4*H) / np.sqrt(D)).astype('f')\n",
    "        rnn_Wh = (rn(H, 4*H) / np.sqrt(H)).astype('f')\n",
    "        rnn_b = np.zeros(4*H).astype('f')\n",
    "        affine_W = (rn(H, 1) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(1).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.affine_layer = Affine(affine_W, affine_b)\n",
    "        self.loss_layer = SigmoidWithLoss()\n",
    "        self.rnn_layer = TimeLSTM(D, H, stateful=False)\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        self.params += self.rnn_layer.params\n",
    "        self.grads += self.rnn_layer.grads\n",
    "        self.params += self.affine_layer.params\n",
    "        self.grads += self.affine_layer.grads\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        xs = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(xs, ts)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, xs):\n",
    "        xs = self.rnn_layer.forward(xs)[######問4.1.2######] \n",
    "        xs = self.affine_layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        dout = self.affine_layer.backward(dout)\n",
    "        dout = self.rnn_layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.rnn_layer.reset_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OXDRZyAX-A0e"
   },
   "source": [
    "## 学習の実行\n",
    "今回のタスクにおいては、RNNではうまく学習が行われず損失があまり減少してくれません。\n",
    "\n",
    "対してLSTMでは、環境によって結果に差は出ますが、RNNに比べると順調に損失が減少します。\n",
    "\n",
    "学習の実行には少々時間がかかるため、注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m7GontpJ-A0f"
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "n_epoch = 10\n",
    "eval_interval = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jl4z5nq7-A0h",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rnn_model = SimpleRNNClassifier(25, 100)\n",
    "lstm_model = LSTMClassifier(25, 100)\n",
    "optimizer1 = Adam(lr)\n",
    "optimizer2 = Adam(lr)\n",
    "batch_size = 100\n",
    "rnn_loss_list = []\n",
    "lstm_loss_list = []\n",
    "\n",
    "np.random.seed(0)\n",
    "for epoch in range(n_epoch):\n",
    "    total_rnn_loss = 0\n",
    "    total_lstm_loss = 0\n",
    "    perm = np.random.permutation(len(X_train))\n",
    "    for i, idx in enumerate(range(0, len(X_train), batch_size)):\n",
    "        X_batch = X_train[perm[idx:idx+batch_size]]\n",
    "        T_batch = T_train[perm[idx:idx+batch_size]]\n",
    "        \n",
    "        rnn_loss = rnn_model.forward(X_batch, T_batch)\n",
    "        rnn_model.backward()\n",
    "        optimizer1.update(rnn_model.params, rnn_model.grads)\n",
    "        total_rnn_loss += rnn_loss*len(X_batch)\n",
    "        \n",
    "        lstm_loss = lstm_model.forward(X_batch, T_batch)\n",
    "        lstm_model.backward()\n",
    "        optimizer2.update(lstm_model.params, lstm_model.grads)\n",
    "        total_lstm_loss += lstm_loss*len(X_batch)\n",
    "        if i % eval_interval == 0:\n",
    "            print('| idx %d / %d | RNN loss %.2f | LSTM loss %.2f |'\n",
    "                 %(idx, len(X_train), total_rnn_loss/(idx+batch_size), total_lstm_loss/(idx+batch_size)))\n",
    "    average_rnn_loss = total_rnn_loss / len(X_train)\n",
    "    rnn_loss_list.append(average_rnn_loss)\n",
    "    average_lstm_loss = total_lstm_loss / len(X_train)\n",
    "    lstm_loss_list.append(average_lstm_loss)\n",
    "    rnn_pred = rnn_model.predict(X_test).flatten()\n",
    "    lstm_pred = lstm_model.predict(X_test).flatten()\n",
    "    rnn_accuracy = ((rnn_pred > 0) == T_test).mean() * 100\n",
    "    lstm_accuracy = ((lstm_pred > 0) == T_test).mean() * 100\n",
    "    print('| epoch %d | RNN loss %.2f | LSTM loss %.2f | RNN accuracy %.2f | LSTM accuracy %.2f'\n",
    "          % (epoch+1, average_rnn_loss, average_lstm_loss, rnn_accuracy, lstm_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3HhOzPJ0-A0k"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(n_epoch), rnn_loss_list)\n",
    "plt.plot(range(n_epoch), lstm_loss_list)\n",
    "plt.legend([\"RNN\", \"LSTM\"])\n",
    "plt.xlabel(\"#epoch\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4m_igEHD-A0l"
   },
   "source": [
    "## bi-directional LSTM\n",
    "問4-2. <font color=\"Red\">以下の BidirectionalLSTMClassifier クラスを完成させてください。</font>\n",
    "\n",
    "入力系列データに対して、順方向のデータを処理するforward LSTM層と逆方向のデータを処理するbackward LSTM層を用意します。\n",
    "\n",
    "通常のLSTM層とパラメータ数が大きく変わらないように各LSTM層は hidden_size の半分の次元数を出力します。\n",
    "\n",
    "forward LSTM と backward LSTM の出力を横につなげ、affine層に入力し分類タスクを解きます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgpTdphW-A0m"
   },
   "outputs": [],
   "source": [
    "class BidirectionalLSTMClassifier:\n",
    "    def __init__(self, wordvec_size, hidden_size):\n",
    "        D, H = wordvec_size, hidden_size //2\n",
    "        rn = np.random.randn\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # 重みの初期化\n",
    "        affine_W = (rn(H * 2, 1) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(1).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.affine_layer = Affine(affine_W, affine_b)\n",
    "        self.loss_layer = SigmoidWithLoss()\n",
    "        self.forward_rnn_layer = TimeLSTM(D, H, stateful=False)\n",
    "        self.backward_rnn_layer = TimeLSTM(D, H, stateful=False)\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        self.params += self.forward_rnn_layer.params\n",
    "        self.grads += self.forward_rnn_layer.grads\n",
    "        self.params += self.backward_rnn_layer.params\n",
    "        self.grads += self.backward_rnn_layer.grads\n",
    "        self.params += self.affine_layer.params\n",
    "        self.grads += self.affine_layer.grads\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        xs = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(xs, ts)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, xs):\n",
    "        forward_xs = self.forward_rnn_layer.forward(xs)[######問4.2.1######] \n",
    "        backward_xs = self.backward_rnn_layer.forward(xs[######問4.2.2######])[######問4.2.3######] \n",
    "        xs = np.hstack((forward_xs, backward_xs))\n",
    "        xs = self.affine_layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        dout = self.affine_layer.backward(dout)\n",
    "        dout1, dout2 = np.hsplit(dout, 2)\n",
    "        dout1 = self.forward_rnn_layer.backward(dout1)\n",
    "        dout2 = self.backward_rnn_layer.backward(dout2)\n",
    "        return (dout1, dout2)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.forward_rnn_layer.reset_state()\n",
    "        self.backward_rnn_layer.reset_state()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "74sZCjOy-A0n"
   },
   "source": [
    "## 学習の実行\n",
    "Bidirectional LSTM がLSTMより良い性能を出していることを確認してください。\n",
    "\n",
    "10エポックで損失が0.2を下回っていれば学習成功です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r1OwKNvA-A0o"
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "n_epoch = 10\n",
    "eval_interval = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JNRv0_2K-A0p",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bilstm_model = BidirectionalLSTMClassifier(25, 100)\n",
    "optimizer = Adam(lr)\n",
    "batch_size = 100\n",
    "bilstm_loss_list = []\n",
    "np.random.seed(0)\n",
    "for epoch in range(n_epoch):\n",
    "    total_bilstm_loss = 0\n",
    "    perm = np.random.permutation(len(X_train))\n",
    "    for i, idx in enumerate(range(0, len(X_train), batch_size)):\n",
    "        X_batch = X_train[perm[idx:idx+batch_size]]\n",
    "        T_batch = T_train[perm[idx:idx+batch_size]]\n",
    "        \n",
    "        bilstm_loss = bilstm_model.forward(X_batch, T_batch)\n",
    "        bilstm_model.backward()\n",
    "        optimizer.update(bilstm_model.params, bilstm_model.grads)\n",
    "        total_bilstm_loss += bilstm_loss*len(X_batch)\n",
    "        if i % eval_interval == 0:\n",
    "            print('| idx %d / %d | BiLSTM loss %.2f |'\n",
    "                 %(idx, len(X_train), total_bilstm_loss/(idx+batch_size)))\n",
    "\n",
    "    average_bilstm_loss = total_bilstm_loss / len(X_train)\n",
    "    bilstm_loss_list.append(average_bilstm_loss)\n",
    "    bilstm_pred = bilstm_model.predict(X_test).flatten()\n",
    "    bilstm_accuracy = ((bilstm_pred > 0) == T_test).mean() * 100\n",
    "    print('| epoch %d | BiLSTM loss %.2f | BiLSTM accuracy %.2f'\n",
    "          % (epoch+1, average_bilstm_loss, bilstm_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TNat1zIf-A0r",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(n_epoch), rnn_loss_list)\n",
    "plt.plot(range(n_epoch), lstm_loss_list)\n",
    "plt.plot(range(n_epoch), bilstm_loss_list)\n",
    "plt.legend([\"RNN\", \"LSTM\", \"BiLSTM\"])\n",
    "plt.xlabel(\"#epoch\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NO6x3b-t-A0s"
   },
   "source": [
    "\n",
    "## 提出可否"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ie0QOeJt-A0s"
   },
   "outputs": [],
   "source": [
    "if average_bilstm_loss > 0.2:\n",
    "    print(\"RNNの実装に間違いがあります。問4を見直してください。\")\n",
    "else:\n",
    "    print(\"学習成功です。次のステップに進んでください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IlswOIfWCvFR"
   },
   "source": [
    "## Seq2Seq\n",
    "## データセット用意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HOzLPGCk-A0u"
   },
   "source": [
    "Seq2seq を用いてタスクを解いていきます。\n",
    "\n",
    "今回用いるデータセットは、足し算の数式を並べたものになります。数式とその答を全て文字列として考え、足し算の式をLSTMによって一文字ずつ読み込み、その答えを一文字ずつ出力していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "000wqFK6CvFS"
   },
   "outputs": [],
   "source": [
    "id_to_char = {}\n",
    "char_to_id = {}\n",
    "\n",
    "\n",
    "def _update_vocab(txt):\n",
    "    chars = list(txt)\n",
    "\n",
    "    for i, char in enumerate(chars):\n",
    "        if char not in char_to_id:\n",
    "            tmp_id = len(char_to_id)\n",
    "            char_to_id[char] = tmp_id\n",
    "            id_to_char[tmp_id] = char\n",
    "\n",
    "def load_sequence(file_name='addition.txt'):\n",
    "    file_path = './' + file_name\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print('No file: %s' % file_name)\n",
    "        return None\n",
    "\n",
    "    questions, answers = [], []\n",
    "\n",
    "    for line in open(file_path, 'r'):\n",
    "        idx = line.find('_')\n",
    "        questions.append(line[:idx])\n",
    "        answers.append(line[idx:-1])\n",
    "\n",
    "    # create vocab dict\n",
    "    for i in range(len(questions)):\n",
    "        q, a = questions[i], answers[i]\n",
    "        _update_vocab(q)\n",
    "        _update_vocab(a)\n",
    "\n",
    "    # create np array\n",
    "    x = np.zeros((len(questions), len(questions[0])), dtype=np.int)\n",
    "    t = np.zeros((len(questions), len(answers[0])), dtype=np.int)\n",
    "\n",
    "    for i, sentence in enumerate(questions):\n",
    "        x[i] = [char_to_id[c] for c in list(sentence)]\n",
    "    for i, sentence in enumerate(answers):\n",
    "        t[i] = [char_to_id[c] for c in list(sentence)]\n",
    "\n",
    "    # shuffle\n",
    "    indices = np.arange(len(x))\n",
    "    np.random.shuffle(indices)\n",
    "    x = x[indices]\n",
    "    t = t[indices]\n",
    "\n",
    "    # 10% for validation set\n",
    "    split_at = len(x) - len(x) // 10\n",
    "    (x_train, x_test) = x[:split_at], x[split_at:]\n",
    "    (t_train, t_test) = t[:split_at], t[split_at:]\n",
    "\n",
    "    return (x_train, t_train), (x_test, t_test)\n",
    "\n",
    "\n",
    "def get_vocab():\n",
    "    return char_to_id, id_to_char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3uFQosKWCvFT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_sequence('addition.txt')\n",
    "char_to_id, id_to_char = get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djem-PB6CvFV"
   },
   "source": [
    "## ネットワーク定義\n",
    "問5. <font color=\"Red\">以下の Seq2seq クラスを完成させてください。</font>\n",
    "\n",
    "  - Encoderクラスは、各時刻での入力を順伝播させた後、最後の時刻に対応する出力をDecoder クラスに渡します。\n",
    "  - Decoderクラスの順伝播では、まずEncoderクラスの出力を内部状態としてセットし、入力系列データを順伝播させていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9TZQsu3QCvFV"
   },
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    " \n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(D, H, stateful=False)\n",
    "\n",
    "        self.params = self.embed.params + self.lstm.params\n",
    "        self.grads = self.embed.grads + self.lstm.grads\n",
    "        self.hs = None\n",
    "\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        self.hs = hs\n",
    "        return hs[######問4.1######]\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout\n",
    "\n",
    "\n",
    "class Decoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(D, H, stateful=True)\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in (self.embed, self.lstm, self.affine):\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, h):\n",
    "        self.lstm.set_state(######問4.2######)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        out = self.lstm.forward(out)\n",
    "        score = self.affine.forward(out)\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        dout = self.lstm.backward(dout)\n",
    "        dout = self.embed.backward(dout)\n",
    "        dh = self.lstm.dh\n",
    "        return dh\n",
    "\n",
    "class Seq2seq():\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = Encoder(V, D, H)\n",
    "        self.decoder = Decoder(V, D, H)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]\n",
    "\n",
    "        h = self.encoder.forward(xs) \n",
    "        score = self.decoder.forward(decoder_xs, h) \n",
    "        loss = self.softmax.forward(score, decoder_ts) \n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.softmax.backward(dout)\n",
    "        dh = self.decoder.backward(dout)\n",
    "        dout = self.encoder.backward(dh)\n",
    "        return dout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xnZqVX0ECvFW"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hideen_size = 128\n",
    "batch_size = 128\n",
    "max_epoch = 8\n",
    "max_grad = 5.0\n",
    "\n",
    "model = Seq2seq(vocab_size, wordvec_size, hideen_size)\n",
    "optimizer = Adam()\n",
    "\n",
    "data_size = len(x_train)\n",
    "max_iters = data_size // batch_size\n",
    "loss_list = []\n",
    "eval_interval = 50\n",
    "current_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gkmNOSrrCvFY"
   },
   "source": [
    "### 学習、評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Shwm9X8OCvFZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    # シャッフル\n",
    "    idx = np.random.permutation(np.arange(data_size))\n",
    "    x = x_train[idx]\n",
    "    t = t_train[idx]\n",
    "    for iters in range(max_iters):\n",
    "        batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "        batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "        # 勾配を求め、パラメータを更新\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "#         params, grads = remove_duplicate(model.params, model.grads)  # 共有された重みを1つに集約\n",
    "        if max_grad is not None:\n",
    "            clip_grads(model.grads, max_grad)\n",
    "        optimizer.update(model.params,model.grads)\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "        # 評価\n",
    "        if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "            avg_loss = total_loss / loss_count\n",
    "            print('| epoch %d |  iter %d / %d | loss %.2f'\n",
    "                  % (current_epoch + 1, iters + 1, max_iters, avg_loss))\n",
    "            loss_list.append(float(avg_loss))\n",
    "            total_loss, loss_count = 0, 0\n",
    "    current_epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P5TtdA2_-A05",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(loss_list)), loss_list)\n",
    "plt.xlabel(\"#iter\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5m6Hk71s-A1D"
   },
   "source": [
    "\n",
    "## 提出可否"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dI9k0bSr-A1D"
   },
   "outputs": [],
   "source": [
    "if loss_list[-1] > 1.1:\n",
    "    print(\"Seq2seqの実装に間違いがあります。問4を見直してください。\")\n",
    "else:\n",
    "    print(\"学習成功です。提出してください\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Day5演習.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
